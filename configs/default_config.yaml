# Default configuration for Phage-Host Interaction Prediction Model

# Data paths
data:
  data_path: "data/dedup.phage_marker_rbp_with_phage_entropy.tsv"
  splits_path: "data/processed/splits.pkl"
  embeddings_path: "data/embeddings"
  output_dir: "outputs"
  checkpoint_dir: "checkpoints"

# Model architecture
model:
  # Embedding dimension from ESM-2
  input_dim: 1280
  
  # Encoder architecture options
  encoder_type: "balanced"  # Options: "conservative", "balanced", "aggressive"
  
  # Balanced encoder dimensions (1280 -> 768 -> 512 -> 256)
  encoder_dims_balanced: [768, 512, 256]
  
  # Conservative encoder dimensions (1280 -> 1024 -> 512)
  encoder_dims_conservative: [1024, 512]
  
  # Aggressive encoder dimensions (1280 -> 512 -> 256 -> 128)
  encoder_dims_aggressive: [512, 256, 128]
  
  # Final embedding dimension (last layer of encoder)
  embedding_dim: 256  # Will be overridden based on encoder_type
  
  # Dropout rate
  dropout: 0.1
  
  # Temperature for sigmoid in pairwise scoring
  # Higher temperature = lower initial probabilities
  temperature: 5.0
  
  # Use layer normalization
  use_layer_norm: true
  
  # Activation function
  activation: "relu"  # Options: "relu", "gelu", "tanh"

# Training parameters
training:
  # Batch size
  batch_size: 32
  
  # Gradient accumulation steps (effective batch size = batch_size * gradient_accumulation_steps)
  gradient_accumulation_steps: 1  # Set to 2, 4, or 8 for larger effective batch sizes
  
  # Learning rate
  learning_rate: 1.0e-4
  
  # Number of epochs
  num_epochs: 100
  
  # Optimizer
  optimizer: "adamw"  # Options: "adam", "adamw", "sgd"
  
  # Weight decay (L2 regularization)
  weight_decay: 1.0e-5
  
  # Gradient clipping
  gradient_clip: 1.0
  
  # Learning rate scheduler
  scheduler: "warmup_cosine"  # Options: "warmup_cosine", "onecycle", "cosine", "exponential", "polynomial", "step", "plateau", "none"
  
  # Scheduler parameters
  scheduler_params:
    # For warmup_cosine (RECOMMENDED for PU learning)
    warmup_epochs: 5  # Number of epochs for linear warmup
    warmup_steps: 1000  # Or specify exact steps (overrides warmup_epochs)
    
    # For onecycle
    pct_start: 0.3  # Percentage of training for increasing LR
    anneal_strategy: "cos"  # "cos" or "linear"
    div_factor: 25.0  # Initial LR = max_lr / div_factor
    final_div_factor: 10000.0  # Final LR = initial_lr / final_div_factor
    
    # For exponential
    gamma: 0.95  # Multiplicative factor per epoch
    
    # For polynomial
    power: 1.0  # Polynomial power (1.0 = linear decay)
    
    # For step scheduler
    step_size: 30
    gamma: 0.1
    
    # For plateau scheduler (NOT RECOMMENDED for PU learning)
    patience: 10
    factor: 0.5
    
    # For cosine scheduler
    T_max: 100
    eta_min: 1.0e-6
  
  # Early stopping
  early_stopping: true
  early_stopping_patience: 20
  early_stopping_min_delta: 1.0e-4
  
  # Checkpoint saving
  save_every_n_epochs: 5
  save_best_only: true
  
  # Validation frequency
  validate_every_n_epochs: 1
  
  # Model calibration (post-training)
  calibrate_model: true  # Whether to calibrate model after training
  calibration_method: "temperature"  # Options: "temperature", "platt"
  calibration_max_iter: 50  # Maximum iterations for calibration optimization
  calibration_lr: 0.01  # Learning rate for calibration optimization

# nnPU Loss parameters
loss:
  # Class prior (proportion of positives in the training data)
  # Since we use 1:1 positive:negative ratio, this should be 0.5
  class_prior: 0.5
  
  # Beta for nnPU loss (controls negative risk correction)
  # Non-zero beta helps with gradient flow
  beta: 0.1
  
  # Gamma for nnPU loss (controls gradient penalty)
  gamma: 1.0
  
  # Use hard bootstrapping
  hard_bootstrap: false

# Data parameters
dataset:
  # Negative sampling ratio (negative:positive)
  # For training: keep balanced for better learning
  negative_ratio: 1.0
  
  # Validation negative ratio (to simulate real-world distribution)
  # 0.2 means 20% positive, 80% negative (closer to reality)
  val_negative_ratio: 4.0
  
  # Test negative ratio (to match real-world ~2% positive rate)
  # 49 negatives per positive = ~2% positive rate
  test_negative_ratio: 49.0
  
  # Random seed for negative sampling
  negative_sampling_seed: 42
  
  # Maximum number of proteins per bag (for padding)
  max_markers: 2
  max_rbps: 3
  
  # Data loader workers
  num_workers: 4
  
  # Pin memory for faster GPU transfer
  pin_memory: true

# Evaluation parameters
evaluation:
  # Metrics to compute
  metrics:
    - "accuracy"
    - "mcc"
    - "f1"
    - "precision"
    - "recall"
    - "auroc"
    - "auprc"
  
  # Compute Hit@K and Recall@K for these values
  k_values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
  
  # Threshold for binary classification
  classification_threshold: 0.5
  
  # Save predictions
  save_predictions: true
  
  # Generate confusion matrix
  generate_confusion_matrix: true

# Logging parameters
logging:
  # Use wandb for experiment tracking
  use_wandb: true
  
  # Wandb project name
  wandb_project: "phage-host-pu-learning"
  
  # Wandb entity (username or team)
  wandb_entity: null  # Set to your wandb username
  
  # Log frequency (batches)
  log_every_n_batches: 10
  
  # Log directory
  log_dir: "logs"
  
  # Verbosity level
  verbosity: "info"  # Options: "debug", "info", "warning", "error"

# Random seeds for reproducibility
seeds:
  # General random seed
  random_seed: 42
  
  # PyTorch seed
  torch_seed: 42
  
  # NumPy seed
  numpy_seed: 42

# Device settings
device:
  # Device to use
  device: "cuda"  # Options: "cuda", "cpu", "mps"
  
  # CUDA device ID (if multiple GPUs)
  cuda_device_id: 0
  
  # Mixed precision training
  use_mixed_precision: false
  
  # Distributed training
  distributed: false
